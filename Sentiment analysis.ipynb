{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras \n",
    "import pandas as pd\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_token = \"UNKOWN_TOKEN\"\n",
    "empty_token   = \"EMPTY_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'glove short',\n",
       " 'glove.42B.300d.txt',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'Sentiment analysis.ipynb',\n",
       " 'test.zip',\n",
       " 'train.zip']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder(address):\n",
    "    '''\n",
    "    adds all the text files in a folder together\n",
    "    '''\n",
    "    \n",
    "    listRes = list()\n",
    "    \n",
    "    for i in os.listdir(address):\n",
    "        with open(address + i, encoding=\"utf8\") as file:\n",
    "            listRes.append(file.read())\n",
    "            \n",
    "    return listRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(neg = \"neg/\", pos=\"pos/\"):\n",
    "    '''\n",
    "    get's data for each type of comment and adds them all together\n",
    "    '''\n",
    "    \n",
    "    listNeg = get_folder(neg)\n",
    "    \n",
    "    listPos = get_folder(pos)\n",
    "    \n",
    "    \n",
    "    return listNeg, listPos\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_dict(w2v):\n",
    "    '''\n",
    "    Creates a dictionary from word2vec files in which the words are the keys and the vectors are the values\n",
    "    '''\n",
    "    dictRes = dict()\n",
    "    for i, iv in enumerate(w2v):\n",
    "        for j, jv in enumerate(iv.split(\"\\n\")):\n",
    "            data = jv.split(\" \")\n",
    "            if len(data[1:]) > 0 :\n",
    "                dictRes[data[0]] = [float(i) for i in data[1:]]\n",
    "            \n",
    "    return dictRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dicts(listNeg, listPos, dictW2v):\n",
    "    \n",
    "    '''\n",
    "    this is a function that gives id to words for later use\n",
    "    w2id : word to id\n",
    "    id2w : id to word\n",
    "    OCC : number of occurrence\n",
    "    '''\n",
    "    \n",
    "    global unknown_token\n",
    "    \n",
    "    W2ID = dict()\n",
    "    ID2W = dict()\n",
    "    \n",
    "    OCC  = dict()\n",
    "    \n",
    "    listAll = listNeg + listPos\n",
    "    \n",
    "    lastID = 0\n",
    "    \n",
    "    for i in listAll :\n",
    "        tokens = word_tokenize(i)\n",
    "        \n",
    "        for j in tokens :\n",
    "            \n",
    "            if j in dictW2v.keys() :\n",
    "                \n",
    "                if not (j in W2ID.keys()):\n",
    "                \n",
    "                    W2ID[j] = lastID\n",
    "                    ID2W[lastID] = j\n",
    "                    lastID += 1\n",
    "                    OCC[j] = 1\n",
    "                    \n",
    "                else:\n",
    "                    OCC[j] +=1\n",
    "                    \n",
    "            else :\n",
    "                if not(unknown_token in W2ID.keys()):\n",
    "                    W2ID[unknown_token] = lastID\n",
    "                    ID2W[lastID] = unknown_token\n",
    "                    lastID += 1\n",
    "                    OCC[unknown_token] = 1\n",
    "                else:\n",
    "                    OCC[unknown_token] += 1\n",
    "    \n",
    "    return W2ID, ID2W, OCC\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spec_data(data, label, w2id, dictW2v):\n",
    "    \n",
    "    '''\n",
    "    for each data , it receives the words and in return puts the vectors of word2vec in their place\n",
    "    if the word is not available in our dictionary we will add an unkown token vector\n",
    "    which was made by a random vector using the variance of the available vectors\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    global unknown_token\n",
    "    \n",
    "    for i in data :\n",
    "        \n",
    "        tokens = word_tokenize(i)\n",
    "        temp = []\n",
    "        for token in tokens :\n",
    "            \n",
    "            if token in w2id.keys():\n",
    "                temp.append(dictW2v[token])\n",
    "            \n",
    "            else :\n",
    "                temp.append(dictW2v[unknown_token])\n",
    "        X.append(np.array(temp))\n",
    "        y.append(label)\n",
    "                \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_data(listNeg, listPos, w2id, id2w, dictW2v):\n",
    "    \n",
    "    '''\n",
    "    concats all data (0 and 1 s) \n",
    "    '''\n",
    "    \n",
    "    Xn, yn = get_spec_data(listNeg, 0, w2id, dictW2v)\n",
    "    \n",
    "    Xp, yp = get_spec_data(listPos, 1, w2id, dictW2v)\n",
    "    \n",
    "    X = Xn + Xp\n",
    "    y = yn + yp\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataForRNN:\n",
    "    \n",
    "    '''\n",
    "    aranges data for the RNN to use\n",
    "    functions will be added in the future for more ease at the time of use\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        \n",
    "        self.dictFinalX = dict()\n",
    "        self.dictFinaly = dict()\n",
    "        \n",
    "        for i, iv in enumerate(X):\n",
    "            \n",
    "            if iv.shape[0] in self.dictFinalX.keys():\n",
    "                \n",
    "                self.dictFinalX[iv.shape[0]].append(iv)\n",
    "                self.dictFinaly[iv.shape[0]].append(y[i])\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.dictFinalX[iv.shape[0]] = [iv]\n",
    "                self.dictFinaly[iv.shape[0]] = [y[i]]\n",
    "            \n",
    "        for i in self.dictFinalX.keys():\n",
    "            \n",
    "            self.dictFinalX[i] = np.array(self.dictFinalX[i]) \n",
    "            self.dictFinaly[i] = np.array(self.dictFinaly[i])\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(shape):\n",
    "    '''\n",
    "    Uses Rnns with LSTM gate for predicting \n",
    "    '''\n",
    "    \n",
    "    in1 = Input(shape)\n",
    "    \n",
    "    X   = LSTM(40, return_sequences=False)(in1)\n",
    "#     X   = LSTM(20, return_sequences=False)(X)\n",
    "#     X   = LSTM(20, return_sequences=False)(X)\n",
    "#     X   = Dense(10, activation=keras.activations.relu)(X)\n",
    "    X   = Dense(10, activation=keras.activations.relu)(X)\n",
    "#     X   = Dense(5, activation=keras.activations.relu)(X)\n",
    "    X   = Dense(5, activation=keras.activations.relu)(X)\n",
    "    X   = Dense(1, activation=keras.activations.sigmoid)(X)\n",
    "    \n",
    "    \n",
    "    model = Model(in1, X)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_cnn(shape):\n",
    "    \n",
    "    '''\n",
    "    creates a model using cnn for predicting\n",
    "    '''\n",
    "    \n",
    "    in1 = Input(shape)\n",
    "    \n",
    "    X   = Conv1D(150, 5, padding=\"SAME\")(in1)\n",
    "    X   = Conv1D(50, 5, strides=2, padding=\"SAME\")(X)\n",
    "    X   = Conv1D(150, 5, padding=\"SAME\")(X)\n",
    "    X   = Conv1D(50, 5, strides=2, padding=\"SAME\")(X)\n",
    "    \n",
    "    \n",
    "    X   = Flatten()(X)\n",
    "    \n",
    "    X   = Dense(100, activation=keras.activations.relu)(X)\n",
    "    X   = Dense(50,  activation=keras.activations.relu)(X)\n",
    "    X   = Dense(50,  activation=keras.activations.relu)(X)\n",
    "    X   = Dense(1,  activation=keras.activations.sigmoid)(X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(in1, X)\n",
    "    \n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_the_same(X, y, empty_vec, mean = True):\n",
    "    \n",
    "    '''\n",
    "    changes the size of all data to one size so we'll be able to use it for CNN\n",
    "    if Mean is true , the sizes are changed into the avg size of the sentences\n",
    "    '''\n",
    "    \n",
    "    global empty_token\n",
    "    \n",
    "    \n",
    "    sizes = np.array([i.shape[0] for i in X])\n",
    "    mean  = np.mean(sizes)\n",
    "    maxSize = np.max(sizes)\n",
    "    \n",
    "    if mean :\n",
    "        size = int(mean)\n",
    "    else:\n",
    "        size = int(maxSize)\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for i, iv in enumerate(X):\n",
    "        \n",
    "        if iv.shape[0] > size :\n",
    "            \n",
    "            res.append(iv[ : size, : ]) \n",
    "            \n",
    "        elif iv.shape[0] < size :\n",
    "            \n",
    "            needsToBeAdded = np.array([ empty_vec for i in range(size - iv.shape[0])])\n",
    "            \n",
    "            res.append(np.concatenate([iv, needsToBeAdded], axis = 0))\n",
    "            \n",
    "            del needsToBeAdded\n",
    "        else :\n",
    "            res.append(iv)\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "listNeg, listPos = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(listNeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(listPos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n"
     ]
    }
   ],
   "source": [
    "print(listPos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '``', 'Teachers', \"''\", '.', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', \"'s\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '``', 'Teachers', \"''\", '.', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '...', '...', '...', 'at', '...', '...', '...', '.', 'High', '.', 'A', 'classic', 'line', ':', 'INSPECTOR', ':', 'I', \"'m\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', ':', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'is', \"n't\", '!']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(listPos[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = get_folder(\"glove short/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictW2v = get_glove_dict(w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the number of words available in our dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictW2v.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see below the NLTK library has done a pretty good job \n",
    "and the things it has separated are also available in our dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"'m\" in dictW2v.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of each vector is 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictW2v[\"the\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode is gonna contain the vector value for each word so we can compute the variance for the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodes = np.array([np.array(dictW2v[i]) for i in dictW2v.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4148703462915355"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodes.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now we haven't considered anything for the unknown tokens and also empty tokens (in case we decide to pad a sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_token in dictW2v.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will Consider it :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_vec = dictW2v[unknown_token] = np.random.uniform(-encodes.var(), +encodes.var(), 50)\n",
    "empty_vec = dictW2v[empty_token] = np.random.uniform(-encodes.var(), +encodes.var(), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2id, id2w, occ = get_dicts(listNeg, listPos, dictW2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have some basic information about our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "838916"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occ[unknown_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289306"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occ[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4738"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occ[\"'m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_final_data(listNeg, listPos, w2id, id2w, dictW2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have our data in vectors but as you can guess comments have diffrent sizes so X.shape has the len of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123, 50)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1195"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([i.shape[0] for i in X]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have more than a 1000 diffrent sizes in comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnData = DataForRNN(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapeEx = rnnData.dictFinalX[list(rnnData.dictFinalX.keys())[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 123, 50)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapeEx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the upward output we see how many comments share the first size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model((None, shapeEx[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 40)                14560     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                410       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 15,031\n",
      "Trainable params: 15,031\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(keras.optimizers.Adam(lr = 1e-3), keras.losses.binary_crossentropy, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(listNeg)\n",
    "del(listPos)\n",
    "del(dictW2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = list(rnnData.dictFinalX.keys())[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in list(rnnData.dictFinalX.keys())[:1]:\n",
    "    example_size = rnnData.dictFinaly[s].shape[0]\n",
    "    if example_size <16 :\n",
    "        batch_size = 2\n",
    "    elif example_size < 128 :\n",
    "        batch_size = 8\n",
    "    else :\n",
    "        batch_size = 16\n",
    "    model.fit(rnnData.dictFinalX[s], rnnData.dictFinaly[s], epochs=30, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to only use the first size because of resources but you can ofcourse use the whole data :D\n",
    "below we see how much our model learned from training datas first size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98        27\n",
      "          1       1.00      0.97      0.98        29\n",
      "\n",
      "avg / total       0.98      0.98      0.98        56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "key = list(rnnData.dictFinalX.keys())[0]\n",
    "y_pred = model.predict(rnnData.dictFinalX[key])\n",
    "print(classification_report(rnnData.dictFinaly[key], y_pred > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "technically we should use diffrent sets for learning and testing but this section was only to show you how to use diffrent sizes for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(rnnData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_data_the_same(X, y, empty_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(282, 50)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.arange(X.shape[0])\n",
    "np.random.shuffle(ind)\n",
    "X = X [ind]\n",
    "\n",
    "y  = y  [ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = get_model_cnn(X.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.compile(keras.optimizers.Adam(lr = 1e-3), keras.losses.binary_crossentropy, metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will see the accuracy for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      "22500/22500 [==============================] - 19s 856us/step - loss: 0.6462 - acc: 0.6338 - val_loss: 0.6284 - val_acc: 0.6372\n",
      "Epoch 2/5\n",
      "22500/22500 [==============================] - 16s 690us/step - loss: 0.5801 - acc: 0.7017 - val_loss: 0.6483 - val_acc: 0.6716\n",
      "Epoch 3/5\n",
      "22500/22500 [==============================] - 16s 697us/step - loss: 0.5659 - acc: 0.7133 - val_loss: 0.5513 - val_acc: 0.7188\n",
      "Epoch 4/5\n",
      "22500/22500 [==============================] - 16s 716us/step - loss: 0.5773 - acc: 0.7068 - val_loss: 0.5680 - val_acc: 0.7132\n",
      "Epoch 5/5\n",
      "22500/22500 [==============================] - 16s 715us/step - loss: 0.5447 - acc: 0.7300 - val_loss: 0.5543 - val_acc: 0.7240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x293ffdd7f28>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0.9\n",
    "row = int(X.shape[0]*p)\n",
    "model_cnn.fit(X[:row], y[:row], epochs=5, batch_size=16, validation_data=[X[row:], y[row:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here the accuracy for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 0.6642 - acc: 0.5611\n",
      "Epoch 2/10\n",
      "900/900 [==============================] - 25s 27ms/step - loss: 0.6580 - acc: 0.5722\n",
      "Epoch 3/10\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 0.6465 - acc: 0.5833\n",
      "Epoch 4/10\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 0.6286 - acc: 0.6000\n",
      "Epoch 5/10\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 0.6181 - acc: 0.6011\n",
      "Epoch 6/10\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 0.6083 - acc: 0.5978\n",
      "Epoch 7/10\n",
      "900/900 [==============================] - 25s 27ms/step - loss: 0.5932 - acc: 0.6233\n",
      "Epoch 8/10\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 0.5688 - acc: 0.6333\n",
      "Epoch 9/10\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 0.5518 - acc: 0.6456\n",
      "Epoch 10/10\n",
      "900/900 [==============================] - 25s 28ms/step - loss: 0.5294 - acc: 0.6544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x293f8edf3c8>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.reset_states()\n",
    "size = 1000\n",
    "row = int(p * size)\n",
    "model.fit(X[:row], y[:row], epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.76      0.58        49\n",
      "          1       0.45      0.20      0.27        51\n",
      "\n",
      "avg / total       0.46      0.47      0.43       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y[row:size], model.predict(X[row:size]) > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see above it takes much more time to train the RNN and even in the same epochs it's not doing as good but you should take into considiration that RNNs can understand more complex patterns <br>\n",
    "if you have time you can use all the data for training and testing use more epochs and also maybe smaller bacth_size for improving the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.s this notebook will be updated soon , thanks for reading :D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
